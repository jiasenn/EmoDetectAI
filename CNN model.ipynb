{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm.notebook import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"train\"\n",
    "TEST_DATA_PATH = \"test\"\n",
    "training_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform = ToTensor())\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform = ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 7178\n",
       "    Root location: test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 3, 48, 48])\n",
      "Shape of y: torch.Size([256]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv7): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=7, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=3,              \n",
    "                out_channels=64,            \n",
    "                kernel_size=3,              \n",
    "                stride=1,                   \n",
    "                padding=1,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),     \n",
    "            nn.ReLU(),                                      \n",
    "        )\n",
    "        self.conv4 = nn.Sequential(         \n",
    "            nn.Conv2d(256, 256, 3, 1, 1),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        \n",
    "        # for 6th layer\n",
    "        self.conv5 = nn.Sequential(         \n",
    "            nn.Conv2d(256, 128, 3, 1, 1),     \n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(         \n",
    "            nn.Conv2d(128, 64, 3, 1, 1),     \n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "        # for 7th layer\n",
    "        self.conv7 = nn.Sequential(         \n",
    "            nn.Conv2d(64, 32, 3, 1, 1),     \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # fully connected layer\n",
    "\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            # change accordingly to number of layers\n",
    "            nn.Linear(6*6*32,7),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.dropout(x)\n",
    "        # flatten \n",
    "        x = x.view(x.size(0), -1)    \n",
    "\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    pbar = tqdm(total=28709//256 + 1)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03599905967712402,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 113,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee188575f4c54ebe99a3bacd1ad88d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.946839  [    0/28709]\n",
      "loss: 1.922046  [25600/28709]\n",
      "Test Error: \n",
      " Accuracy: 24.7%, Avg loss: 1.902490 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03299283981323242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 113,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d1923d4c3942a7852a14c186ebdc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.874938  [    0/28709]\n",
      "loss: 1.877800  [25600/28709]\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 1.893554 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03199410438537598,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 113,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0c8235bf7a4af6aed660648c15b03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.871819  [    0/28709]\n",
      "loss: 1.867665  [25600/28709]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 1.843666 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0279998779296875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 113,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591baf6de9b542b5a2a9ad1706d35511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.843645  [    0/28709]\n",
      "loss: 1.773158  [25600/28709]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.822343 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02899932861328125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 113,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f626a0e694cb48dda6c11ec5cd7de365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.807712  [    0/28709]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\Desktop\\ADL project\\CNN model.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\PC\\Desktop\\ADL project\\CNN model.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39m28709\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m256\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ADL%20project/CNN%20model.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 167\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    168\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 64\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model 7 layer 55 acc.pth\")\n",
    "print(\"Saved PyTorch Model State to modelcnn7.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv7): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=7, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"model 7 layer 55 acc.pth\"))\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.607474 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('cnn')\n",
    "test(test_dataloader, model, loss_fn)\n",
    "# print('mlp')\n",
    "# test(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "filter = model.conv7[0].cpu().weight.data.clone()\n",
    "visTensor(filter, ch=0, allkernels=False)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
